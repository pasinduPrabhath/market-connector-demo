Step 1.1: Find PostgreSQL Configuration File
# Find postgres config location
sudo -u postgres psql -c "SHOW config_file;"
-------------------------------
Step 1.2: Edit PostgreSQL Configuration
sudo nano /etc/postgresql/15/main/postgresql.conf

* Find and modify these lines:
-------------------------------
# Around line 180-200, find:
#wal_level = replica
# Change to:
wal_level = logical

# Around line 260-280, find:
#max_replication_slots = 10
# Change to (or increase to):
max_replication_slots = 10

# Around line 280-300, find:
#max_wal_senders = 10
# Change to:
max_wal_senders = 10

-----------------------------
Step 1.3: Restart PostgreSQL
sudo systemctl restart postgresql
# Verify it restarted successfully
sudo systemctl status postgresql
-------------------------

Step 1.4: Create Replication User (for Debezium)
* debezium needs a special PostgreSQL user with replication privileges:
# Connect to PostgreSQL as admin
sudo -u postgres psql

# Then run these SQL commands:
CREATE ROLE debezium_user WITH LOGIN ENCRYPTED PASSWORD 'debezium_pass';
ALTER ROLE debezium_user REPLICATION;
ALTER ROLE debezium_user SUPERUSER;
\q

--------------------
Step 1.5: Update PostgreSQL Authentication (pg_hba.conf)
sudo nano /etc/postgresql/15/main/pg_hba.conf

Add this line at the END of the file:

# Allow debezium to connect from localhost for replication
host    all             debezium_user   127.0.0.1/32            md5
host    replication     debezium_user   127.0.0.1/32            md5

--------------------

Step 1.6: Restart PostgreSQL Again
sudo systemctl restart postgresql
sudo systemctl status postgresql

--------------------
Step 1.7: Verify Configuration
# Test connection with debezium user
psql -h localhost -U debezium_user -d your_pos_db_name -c "SELECT version();"
# When prompted for password, enter: debezium_pass



üê≥ Step 2: Set Up Docker Containers (Kafka + Debezium)
Step 2.1: Create docker-compose.yml
cd /home/pasindu/Documents/GitHub/market-connector-demo
mkdir -p docker
nano docker/docker-compose.yml

version: '3.8'

services:
  # Zookeeper (required for Kafka)
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: market-connector-zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks:
      - market-connector-network

  # Kafka Message Broker
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: market-connector-kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
    networks:
      - market-connector-network

  # Debezium Connect (CDC Connector)
  debezium:
    image: debezium/connect:2.4.0.Final
    container_name: market-connector-debezium
    depends_on:
      - kafka
    ports:
      - "8083:8083"
    environment:
      BOOTSTRAP_SERVERS: kafka:29092
      GROUP_ID: 1
      CONFIG_STORAGE_TOPIC: connect_configs
      OFFSET_STORAGE_TOPIC: connect_offsets
      STATUS_STORAGE_TOPIC: connect_status
      CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE: 'false'
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: 'false'
    networks:
      - market-connector-network
    # Wait for Kafka to be ready
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8083/"]
      interval: 10s
      timeout: 5s
      retries: 5

  # PostgreSQL for Target Database (Integration Service)
  postgres-integration:
    image: postgres:15-alpine
    container_name: market-connector-postgres-target
    environment:
      POSTGRES_DB: integration_db
      POSTGRES_USER: integration_user
      POSTGRES_PASSWORD: integration_pass
    ports:
      - "5433:5432"
    volumes:
      - ./postgres/init-target-db.sql:/docker-entrypoint-initdb.d/init.sql
    networks:
      - market-connector-network

networks:
  market-connector-network:
    driver: bridge

-------------
Step 2.2: Create Target Database Init Script
nano docker/postgres/init-target-db.sql

-- Integration Service Database Schema

-- Cached inventory table (synced from POS)
CREATE TABLE IF NOT EXISTS inventory_cache (
    id BIGSERIAL PRIMARY KEY,
    product_id VARCHAR(100) NOT NULL,
    sku VARCHAR(100) NOT NULL,
    quantity INTEGER NOT NULL,
    store_id VARCHAR(50),
    last_synced_at TIMESTAMP DEFAULT NOW(),
    cdc_event_timestamp TIMESTAMP,
    UNIQUE(product_id, store_id)
);

-- Indexes for performance
CREATE INDEX IF NOT EXISTS idx_inventory_cache_sku ON inventory_cache(sku);
CREATE INDEX IF NOT EXISTS idx_inventory_cache_product_id ON inventory_cache(product_id);
CREATE INDEX IF NOT EXISTS idx_inventory_cache_last_synced ON inventory_cache(last_synced_at);

-- Sync monitoring/logging
CREATE TABLE IF NOT EXISTS sync_log (
    id BIGSERIAL PRIMARY KEY,
    timestamp TIMESTAMP DEFAULT NOW(),
    table_name VARCHAR(100),
    operation VARCHAR(20),
    record_id VARCHAR(100),
    status VARCHAR(20),
    error_message TEXT,
    event_timestamp BIGINT
);

CREATE INDEX IF NOT EXISTS idx_sync_log_timestamp ON sync_log(timestamp);
CREATE INDEX IF NOT EXISTS idx_sync_log_status ON sync_log(status);

-- Third-party SKU mappings (for future use)
CREATE TABLE IF NOT EXISTS third_party_mappings (
    id BIGSERIAL PRIMARY KEY,
    internal_sku VARCHAR(100) NOT NULL,
    third_party VARCHAR(50) NOT NULL,
    external_sku VARCHAR(100) NOT NULL,
    created_at TIMESTAMP DEFAULT NOW(),
    UNIQUE(internal_sku, third_party)
);

GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO integration_user;
GRANT USAGE ON SCHEMA public TO integration_user;

---------------------
Step 2.3: Start Docker Containers
cd /home/pasindu/Documents/GitHub/market-connector-demo/docker

# Start all services
docker-compose up -d

# Verify all containers are running
docker-compose ps

# Check logs
docker-compose logs -f


-------------------------------------------------
üîó Step 3: Configure Debezium Connector

nano docker/debezium-connector.json

{
  "name": "pos-inventory-connector",
  "config": {
    "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
    "database.hostname": "host.docker.internal",
    "database.port": 5432,
    "database.user": "debezium_user",
    "database.password": "debezium_pass",
    "database.dbname": "your_actual_pos_db_name",
    "database.server.name": "pos",
    "table.include.list": "public.inventory",
    "plugin.name": "pgoutput",
    "publication.name": "dbz_publication",
    "snapshot.mode": "initial",
    "publication.autocreate.mode": "filtered",
    "column.include.list": "public.inventory.inventoryid,public.inventory.productitem,public.inventory.quantity,public.inventory.fromlocation,public.inventory.tolocation,public.inventory.frombin,public.inventory.tobin,public.inventory.currentcost,public.inventory.retailsalesprice,public.inventory.list,public.inventory.movementdate,public.inventory.transactioncode,public.inventory.transactionnumber,public.inventory.intransit,public.inventory.offline,public.inventory.reserveinventory,public.inventory.organisation_id,public.inventory.createdatetime",
    "transforms": "route,unwrap",
    "transforms.route.type": "org.apache.kafka.connect.transforms.RegexRouter",
    "transforms.route.regex": "([^.]+)\\.([^.]+)\\.([^.]+)",
    "transforms.route.replacement": "$1.$2.$3",
    "transforms.unwrap.type": "io.debezium.transforms.ExtractNewRecordState",
    "transforms.unwrap.drop.tombstones": false,
    "transforms.unwrap.delete.handling.mode": "rewrite",
    "transforms.unwrap.add.fields": "op,ts_ms",
    "key.converter": "org.apache.kafka.connect.json.JsonConverter",
    "key.converter.schemas.enable": false,
    "value.converter": "org.apache.kafka.connect.json.JsonConverter",
    "value.converter.schemas.enable": false,
    "heartbeat.interval.ms": 10000
  }
}

-----------------------------
‚ö†Ô∏è IMPORTANT - Replace these values:

database.hostname: If running Docker on same machine, use host.docker.internal (Linux) or localhost (Docker Desktop Windows/Mac)
database.dbname: Your actual POS database name
table.include.list: The tables you want to sync (comma-separated)

Step 3.2: Register Debezium Connector

# Submit connector configuration to Debezium
curl -X POST http://localhost:8083/connectors \
  -H "Content-Type: application/json" \
  -d @docker/debezium-connector.json


‚úÖ If you see this ‚Üí Debezium connector registered successfully!

Step 3.3: Verify Connector Status
# Check connector status
curl http://localhost:8083/connectors/pos-inventory-connector/status

# List all active connectors
curl http://localhost:8083/connectors

-------------------------
üì° Step 4: Verify Kafka Topics Created
Debezium automatically creates Kafka topics. Verify they exist:

# Enter Kafka container
docker exec -it market-connector-kafka bash

# List topics
kafka-topics --bootstrap-server kafka:9092 --list

# You should see:
# - pos.public.inventory
# - pos.public.products
# (or whatever tables you configured)

# Read messages from topic (to test)
kafka-console-consumer --bootstrap-server kafka:9092 \
  --topic pos.public.inventory \
  --from-beginning

-----------------------------
üß™ Step 5: Test the CDC Flow Manually
Step 5.1: Update Data in Your POS Database
# Connect to your local POS database
psql -h localhost -U your_pos_user -d your_pos_db_name

# Then run a test update:
UPDATE inventory SET quantity = 99 WHERE product_id = 'SKU123';
------------
Step 5.2: Check Kafka for Messages
# In Kafka container, consume the topic
docker exec -it market-connector-kafka bash

kafka-console-consumer --bootstrap-server kafka:9092 \
  --topic pos.public.inventory \
  --from-beginning \
  --property print.key=true

---------------------
üõ†Ô∏è Step 6: Update Your Java Application Configuration
Now update your Java app to connect properly:

Step 6.1: Update persistence.xml

<!-- In sourcePU section, change: -->
<property name="jakarta.persistence.jdbc.url" value="jdbc:postgresql://localhost:5432/your_pos_db_name"/>
<property name="jakarta.persistence.jdbc.user" value="your_pos_user"/>
<property name="jakarta.persistence.jdbc.password" value="your_pos_password"/>

<!-- In targetPU section, change: -->
<property name="jakarta.persistence.jdbc.url" value="jdbc:postgresql://localhost:5433/integration_db"/>
<property name="jakarta.persistence.jdbc.user" value="integration_user"/>
<property name="jakarta.persistence.jdbc.password" value="integration_pass"/>


----------------------
Step 6.2: Update Kafka Consumer Configuration

// In createConsumer() method:
props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
props.put(ConsumerConfig.GROUP_ID_CONFIG, "market-connector-demo-group");
props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");


üöÄ Complete Workflow Now

1. ‚úÖ PostgreSQL (local) configured for logical replication
        ‚Üì
2. ‚úÖ Docker containers running (Kafka, Debezium, Target DB)
        ‚Üì
3. ‚úÖ Debezium connector registered to watch your POS tables
        ‚Üì
4. ‚úÖ Changes in POS DB ‚Üí Debezium captures ‚Üí Kafka topic
        ‚Üì
5. ‚úÖ Your Java app reads from Kafka ‚Üí Saves to Target DB
        ‚Üì
6. ‚úÖ REST API returns synced data



see kafka logs
 docker exec -it market-connector-kafka kafka-console-consumer \
  --bootstrap-server kafka:9092 \
  --topic pos.public.inventory \
  --from-beginning \
  --max-messages 5

===================================
Monitoring

# Check Debezium source connector status
curl http://localhost:8083/connectors/pos-inventory-connector/status | jq

# Check JDBC sink connector status
curl http://localhost:8083/connectors/jdbc-sink-inventory/status | jq

# View Kafka topic content
docker exec -it market-connector-kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic pos.public.inventory \
  --from-beginning \
  --max-messages 10

-----------------------------------
üß™ Testing Real-Time CDC
After the initial snapshot completes, test real-time sync:

# Connect to your POS database
psql -h 192.168.1.74 -U debezium_user -d octopus_retaildb

# Update a record
UPDATE inventory SET quantity = 999 WHERE inventoryid = 123;

# Check Kafka immediately (should see update within 1-2 seconds)
docker exec -it market-connector-kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic pos.public.inventory \
  --from-beginning \
  --max-messages 1

# Check target DB in DBeaver
SELECT * FROM inventory_cache WHERE inventoryid = 123;
-- quantity should now be 999